<!DOCTYPE html>
<html>
<head>
    <title>Communication Aid (Pose Data Fix)</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/pose@0.8/dist/teachablemachine-pose.min.js"></script>
    
    <style>
      /* Styling for clear visibility and centered layout */
      body {
          font-family: Arial, sans-serif;
          display: flex;
          flex-direction: column;
          align-items: center;
          margin-top: 20px;
      }
      #webcam-container {
        /* Hides the canvas used for prediction */
        display: none; 
      }
      #video-display-container {
        width: 300px; 
        height: 200px;
        border: 4px solid #CC0000; 
        margin: 10px auto;
        position: relative;
        overflow: hidden;
      }
      #video-element {
        width: 100%;
        height: 100%;
        object-fit: cover;
        /* Flip the video horizontally to mirror the user */
        transform: scaleX(-1); 
      }
      #label-container {
        font-size: 32px; 
        font-weight: bold;
        text-align: center;
        color: #CC0000; 
        margin-top: 15px;
        width: 300px;
      }
    </style>
</head>
<body>
    
    <div id="video-display-container">
        <video id="video-element" autoplay playsinline muted></video>
    </div>
    
    <div id="webcam-container"></div>
    
    <div id="label-container">LOADING MODEL...</div>
    
    <script type="text/javascript">
        // --- MODEL CONFIGURATION ---
        const URL = "https://teachablemachine.withgoogle.com/models/qLMkiB_Zn/"; 
        const COMMUNICATION_MESSAGE = "I need help, please!";
        // Lowered for reliable testing after the initial crash
        const CONFIDENCE_THRESHOLD = 0.50; 

        let model, labelContainer, maxPredictions;
        let messageSpoken = false; 
        let video;

        function speakMessage() {
            if ('speechSynthesis' in window) {
                const utterance = new SpeechSynthesisUtterance(COMMUNICATION_MESSAGE);
                window.speechSynthesis.speak(utterance);
            }
        }

        async function init() {
            const modelURL = URL + "model.json";
            const metadataURL = URL + "metadata.json";
            labelContainer = document.getElementById("label-container");
            video = document.getElementById("video-element");
            
            try {
                // 1. Load Model (Pose Model)
                model = await tmPose.load(modelURL, metadataURL, { quantization: 'uint8' });
                maxPredictions = model.getTotalClasses();

                // 2. Start standard HTML5 Video Stream
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                
                await new Promise(resolve => {
                    video.onloadedmetadata = () => {
                        // CRITICAL: Ensure the video element starts playing
                        video.play();
                        resolve(video);
                    };
                });
                
                labelContainer.innerHTML = "Ready! Look at the camera.";
                window.requestAnimationFrame(loop);
                
            } catch (error) {
                labelContainer.innerHTML = "ERROR: Cannot start video feed. Check permissions.";
                console.error("Initialization Error:", error);
            }
        }

        function loop() {
            // No need to manually draw canvas here, tmPose handles it internally
            if (!model || !video || video.paused || video.ended) {
                window.requestAnimationFrame(loop);
                return;
            }
            
            predict();
            window.requestAnimationFrame(loop);
        }

        async function predict() {
            // **** CRITICAL FIX HERE ****
            // We use the model.predict(video) function directly, 
            // which tells tmPose to find the pose in the video frame, 
            // extract the 14739 data points, and then predict.
            const { prediction } = await model.predict(video);
            
            let highestPrediction = { className: "Unknown", probability: 0 };
            for (let i = 0; i < maxPredictions; i++) {
                if (prediction[i].probability > highestPrediction.probability) {
                    highestPrediction = { 
                        className: prediction[i].className, 
                        probability: prediction[i].probability 
                    };
                }
            }
            
            labelContainer.innerHTML = highestPrediction.className + " (" + (highestPrediction.probability * 100).toFixed(0) + "%)";

            const isTriggerPose = (
                highestPrediction.className === "Arm UP" ||
                highestPrediction.className === "Head Tilt"
            );

            if (isTriggerPose && highestPrediction.probability > CONFIDENCE_THRESHOLD && !messageSpoken) {
                
                speakMessage();
                messageSpoken = true;
                labelContainer.innerHTML = "ðŸ“¢ **MESSAGE SENT!** ðŸ“¢"; 
                
            } else if (highestPrediction.className === "Neutral" && highestPrediction.probability > CONFIDENCE_THRESHOLD) {
                messageSpoken = false;
            }
        }

        window.addEventListener('load', init);
    </script>
</body>
</html>
