<!DOCTYPE html>
<html>
<head>
    <title>Communication Aid (HTML5 Fix)</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/pose@0.8/dist/teachablemachine-pose.min.js"></script>
    
    <style>
      body {
          font-family: Arial, sans-serif;
          display: flex;
          flex-direction: column;
          align-items: center;
          margin-top: 20px;
      }
      #webcam-container {
        /* CRITICAL: Hides the canvas used for prediction */
        display: none; 
      }
      #video-display-container {
        width: 300px; 
        height: 200px;
        border: 4px solid #CC0000; /* New border to show the active video element */
        margin: 10px auto;
        position: relative;
        overflow: hidden;
      }
      /* Style the standard HTML video element */
      #video-element {
        width: 100%;
        height: 100%;
        object-fit: cover;
        /* Flip the video horizontally to mirror the user */
        transform: scaleX(-1); 
      }
      #label-container {
        font-size: 32px; 
        font-weight: bold;
        text-align: center;
        color: #CC0000; 
        margin-top: 15px;
        width: 300px;
      }
    </style>
</head>
<body>
    
    <div id="video-display-container">
        <video id="video-element" autoplay playsinline muted></video>
    </div>
    
    <div id="webcam-container"></div>
    
    <div id="label-container">LOADING MODEL...</div>
    
    <script type="text/javascript">
        // --- MODEL CONFIGURATION ---
        const URL = "https://teachablemachine.withgoogle.com/models/qLMkiB_Zn/"; 
        const COMMUNICATION_MESSAGE = "I need help, please!";
        const CONFIDENCE_THRESHOLD = 0.80; 

        let model, labelContainer, maxPredictions;
        let messageSpoken = false; 
        let video; // Reference to the HTML <video> element
        let canvas; // Reference to the canvas where prediction happens

        function speakMessage() {
            if ('speechSynthesis' in window) {
                const utterance = new SpeechSynthesisUtterance(COMMUNICATION_MESSAGE);
                window.speechSynthesis.speak(utterance);
            }
        }

        async function init() {
            const modelURL = URL + "model.json";
            const metadataURL = URL + "metadata.json";
            labelContainer = document.getElementById("label-container");
            video = document.getElementById("video-element");
            
            try {
                // 1. Load Model
                model = await tmPose.load(modelURL, metadataURL, { quantization: 'uint8' });
                maxPredictions = model.getTotalClasses();

                // 2. Start standard HTML5 Video Stream
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                
                // Wait for video stream to actually load
                await new Promise(resolve => {
                    video.onloadedmetadata = () => {
                        resolve(video);
                    };
                });
                
                // Create a hidden canvas to feed the video frames to the TM model
                canvas = document.createElement('canvas');
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                document.getElementById("webcam-container").appendChild(canvas);

                labelContainer.innerHTML = "Ready! Look at the camera.";
                window.requestAnimationFrame(loop);
                
            } catch (error) {
                labelContainer.innerHTML = "ERROR: Cannot start video feed. Check permissions.";
                console.error("Initialization Error:", error);
            }
        }

        function loop() {
            if (!model || !video || video.paused || video.ended) {
                window.requestAnimationFrame(loop);
                return;
            }
            
            // Draw the current video frame onto the hidden canvas
            const ctx = canvas.getContext('2d');
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            // Flip the image horizontally on the canvas to match the user's view
            ctx.save();
            ctx.scale(-1, 1);
            ctx.drawImage(video, -canvas.width, 0, canvas.width, canvas.height);
            ctx.restore();

            predict();
            window.requestAnimationFrame(loop);
        }

        async function predict() {
            // Predict using the hidden canvas element
            const { prediction } = await model.predict(canvas);
            
            let highestPrediction = { className: "Unknown", probability: 0 };
            for (let i = 0; i < maxPredictions; i++) {
                if (prediction[i].probability > highestPrediction.probability) {
                    highestPrediction = { 
                        className: prediction[i].className, 
                        probability: prediction[i].probability 
                    };
                }
            }
            
            labelContainer.innerHTML = highestPrediction.className + " (" + (highestPrediction.probability * 100).toFixed(0) + "%)";

            const isTriggerPose = (
                highestPrediction.className === "Arm UP" ||
                highestPrediction.className === "Head Tilt"
            );

            if (isTriggerPose && highestPrediction.probability > CONFIDENCE_THRESHOLD && !messageSpoken) {
                
                speakMessage();
                messageSpoken = true;
                labelContainer.innerHTML = "ðŸ“¢ **MESSAGE SENT!** ðŸ“¢"; 
                
            } else if (highestPrediction.className === "Neutral" && highestPrediction.probability > CONFIDENCE_THRESHOLD) {
                messageSpoken = false;
            }
        }

        window.addEventListener('load', init);
    </script>
</body>
</html>
